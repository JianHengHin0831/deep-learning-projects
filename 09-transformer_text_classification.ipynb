{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b62a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b640e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. prepare data ---\n",
    "\n",
    "# load dataset\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "# turn iterator to list to be able to rewind it later\n",
    "train_data = list(train_iter)\n",
    "test_data = list(test_iter)\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# build vocabulary from training set\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# add special tokens <unk> and <pad>\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "PAD_IDX = vocab['<pad>']\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "#define text and label pipelines\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1  # labels are 1, 2, 3, 4 in AG_NEWS\n",
    "\n",
    "# define collate function for DataLoader\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # use pad_sequence to pad the text sequences to the same length\n",
    "    # batch_first=False for transformer input\n",
    "    padded_text_list = pad_sequence(text_list, batch_first=False, padding_value=PAD_IDX)\n",
    "    return label_list.to(device), padded_text_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. define model ---\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # positional encoding module\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    # transformer model for text classification\n",
    "    def __init__(self, vocab_size, embed_dim, nhead, ffn_hid_dim, nlayers, num_class, dropout=0.5):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embed_dim, nhead, ffn_hid_dim, dropout, batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.d_model = embed_dim\n",
    "        self.decoder = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_padding_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_padding_mask)\n",
    "        # average pooling over the sequence dimension to get a fixed-size representation\n",
    "        output = output.mean(dim=0)\n",
    "        return self.decoder(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b621f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5\n",
    "BATCH_SIZE = 64\n",
    "num_class = len(set([label for (label, text) in train_data]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "nhead = 4\n",
    "ffn_hid_dim = 64\n",
    "nlayers = 2\n",
    "EPOCHS = 5\n",
    "\n",
    "# optimizer, loss function and model instantiation\n",
    "model = TransformerClassifier(vocab_size, emsize, nhead, ffn_hid_dim, nlayers, num_class).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# create DataLoader\n",
    "train_dataset, test_dataset = to_map_style_dataset(train_data), to_map_style_dataset(test_data)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. set up training and evaluation functions ---\n",
    "\n",
    "# hyperparameters\n",
    "EPOCHS = 10\n",
    "LR = 5.0  \n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 64      # embedding dimension\n",
    "NHEAD = 4           # number of heads in multi-head attention\n",
    "FFN_HID_DIM = 64    # feedforward network hidden dimension\n",
    "NLAYERS = 2         # Transformer Encoder layers\n",
    "NUM_CLASS = len(set([label for (label, text) in train_data]))\n",
    "\n",
    "# instantiate the model\n",
    "model = TransformerClassifier(VOCAB_SIZE, EMBED_DIM, NHEAD, FFN_HID_DIM, NLAYERS, NUM_CLASS).to(device)\n",
    "\n",
    "# define loss function, optimizer and learning rate scheduler\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34518d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. training and evaluation functions ---\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    \n",
    "    for label, text in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # generate padding mask\n",
    "        src_padding_mask = (text == PAD_IDX).transpose(0, 1)\n",
    "        \n",
    "        predicted_label = model(text, src_padding_mask)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        \n",
    "    return total_acc / total_count\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text in dataloader:\n",
    "            src_padding_mask = (text == PAD_IDX).transpose(0, 1)\n",
    "            predicted_label = model(text, src_padding_mask)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45a467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [07:12<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 443.10s | train accuracy    0.782 | test accuracy    0.897 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [10:25<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 639.11s | train accuracy    0.905 | test accuracy    0.904 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [12:02<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 736.27s | train accuracy    0.911 | test accuracy    0.905 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [13:13<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 807.61s | train accuracy    0.911 | test accuracy    0.906 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [10:21<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 632.15s | train accuracy    0.910 | test accuracy    0.905 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [10:01<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 612.18s | train accuracy    0.911 | test accuracy    0.905 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [10:05<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 617.08s | train accuracy    0.911 | test accuracy    0.905 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [10:25<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 636.00s | train accuracy    0.911 | test accuracy    0.905 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [10:03<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 614.39s | train accuracy    0.911 | test accuracy    0.905 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [06:55<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 418.75s | train accuracy    0.912 | test accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of the best model on test dataset.\n",
      "Test accuracy    0.906\n",
      "\n",
      "--- Prediction Example ---\n",
      "Sample Text: MEMPHIS, Tenn. – Four days ago, Jon Rahm was enduring the season’s worst weather conditions on Sunday at The Open on his way to a closing 75.\n",
      "Predicted category: Sports\n"
     ]
    }
   ],
   "source": [
    "# --- 6. training loop ---\n",
    "\n",
    "# generate DataLoader\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    accu_train = train(train_dataloader)\n",
    "    accu_test = evaluate(test_dataloader)\n",
    "    \n",
    "    if total_accu is None or accu_test > total_accu:\n",
    "        total_accu = accu_test\n",
    "        # save the best model\n",
    "        torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "        \n",
    "    print('-' * 59)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | '\n",
    "          f'train accuracy {accu_train:8.3f} | test accuracy {accu_test:8.3f} ')\n",
    "    print('-' * 59)\n",
    "    scheduler.step()\n",
    "\n",
    "# --- 7. evaluate the model on test dataset ---\n",
    "model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
    "print('Checking the results of the best model on test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print(f'Test accuracy {accu_test:8.3f}')\n",
    "\n",
    "# --- 8. predict a new sentence ---\n",
    "def predict(text, model, vocab, text_pipeline):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tensor = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device).unsqueeze(1)\n",
    "        output = model(tensor)\n",
    "        return output.argmax(1).item() + 1 # +1 is because we subtracted 1 from labels earlier\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was enduring the season’s worst weather conditions on Sunday at The Open on his way to a closing 75.\"\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "\n",
    "model = model.to(\"cpu\") \n",
    "print(\"\\n--- Prediction Example ---\")\n",
    "print(\"Sample Text:\", ex_text_str)\n",
    "print(\"Predicted category:\", ag_news_label[predict(ex_text_str, model, vocab, text_pipeline)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
